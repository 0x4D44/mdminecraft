# Metrics Diff Tool - Usage Guide

**Tool:** `metrics-diff`
**Purpose:** Compare performance metrics and detect regressions
**Location:** `crates/cli/src/bin/metrics-diff.rs`
**Use Case:** CI/CD regression detection, performance tracking

---

## Overview

The `metrics-diff` tool compares two metrics JSON files and reports performance changes. It's designed for automated regression detection in CI/CD pipelines.

**Key Features:**
- Compares all metrics between baseline and current
- Calculates percentage changes
- Applies configurable thresholds
- Returns appropriate exit codes for CI
- Multiple output formats (text, JSON)
- Beautiful terminal output

---

## Installation

The tool is built as part of the CLI crate:

```bash
cargo build --package mdminecraft-cli --bin metrics-diff --release
```

The binary will be at: `target/release/metrics-diff`

---

## Basic Usage

### Compare Two Metrics Files

```bash
metrics-diff baseline.json current.json
```

**Output:**
```
╔════════════════════════════════════════════════════════════════╗
║           Metrics Diff Report                                  ║
╚════════════════════════════════════════════════════════════════╝

Baseline:  baseline.json
Current:   current.json

Test:      large_scale_terrain_worldtest → large_scale_terrain_worldtest
Result:    Pass → Pass

Thresholds:
  Warning:  5%
  Failure:  10%

┌────────────────────────────────────┬──────────────┬──────────────┬──────────┬────────┐
│ Metric                             │ Baseline     │ Current      │ Change   │ Status │
├────────────────────────────────────┼──────────────┼──────────────┼──────────┼────────┤
│ terrain.avg_gen_time_us            │     4400.000 │     4450.000 │    1.14% │ ✅ PASS │
│ terrain.chunks_per_second          │      228.000 │      225.000 │   -1.32% │ ✅ PASS │
│ terrain.max_gen_time_us            │     7000.000 │     7500.000 │    7.14% │ ⚠️  WARN │
│ execution.duration_seconds         │       15.010 │       16.800 │   11.92% │ ❌ FAIL │
└────────────────────────────────────┴──────────────┴──────────────┴──────────┴────────┘

Summary:
  ✅ Passed:  2/4
  ⚠️  Warnings: 1/4
  ❌ Failed:  1/4

❌ FAILURE: Performance regressions detected (>10%)
```

---

## Command-Line Options

### Output Format

**Text (default):**
```bash
metrics-diff baseline.json current.json
```

**JSON (for CI parsing):**
```bash
metrics-diff baseline.json current.json --format json
```

JSON output example:
```json
{
  "metrics": [
    {
      "name": "terrain.avg_gen_time_us",
      "baseline": 4400.0,
      "current": 4450.0,
      "change_percent": 1.14,
      "status": "PASS"
    }
  ],
  "summary": {
    "total": 4,
    "passed": 2,
    "warnings": 1,
    "failures": 1
  }
}
```

### Custom Thresholds

**Warning Threshold (default: 5%):**
```bash
metrics-diff baseline.json current.json --threshold-warning 0.03
```

**Failure Threshold (default: 10%):**
```bash
metrics-diff baseline.json current.json --threshold-failure 0.08
```

**Both:**
```bash
metrics-diff baseline.json current.json \
  --threshold-warning 0.03 \
  --threshold-failure 0.08
```

---

## Exit Codes

The tool uses exit codes suitable for CI integration:

| Exit Code | Meaning | Description |
|-----------|---------|-------------|
| 0 | **Success** | All metrics within acceptable ranges |
| 1 | **Warning** | Some metrics degraded 5-10% |
| 2 | **Failure** | Some metrics degraded >10% OR errors occurred |

**Example CI usage:**
```bash
#!/bin/bash
metrics-diff baseline.json current.json
EXIT_CODE=$?

if [ $EXIT_CODE -eq 0 ]; then
    echo "✅ Performance check passed"
elif [ $EXIT_CODE -eq 1 ]; then
    echo "⚠️  Performance warnings detected"
    exit 0  # Or decide to fail the build
else
    echo "❌ Performance regressions detected"
    exit 1  # Fail the build
fi
```

---

## Metrics Compared

The tool compares the following metrics when present:

### Terrain Metrics
- `avg_gen_time_us` - Average terrain generation time (lower is better)
- `chunks_per_second` - Generation throughput (higher is better)
- `max_gen_time_us` - Peak generation time (lower is better)

### Mob Metrics
- `avg_update_time_us` - Average mob update time (lower is better)
- `total_spawned` - Total mobs spawned (equality expected for determinism)

### Item Metrics
- `avg_update_time_us` - Average item update time (lower is better)

### Persistence Metrics
- `avg_save_time_us` - Average save time (lower is better)
- `compression_ratio` - Compression effectiveness (higher is better)

### Execution Metrics
- `duration_seconds` - Total test duration (lower is better)

---

## CI/CD Integration

### GitHub Actions Example

```yaml
name: Performance Regression Check

on: [pull_request]

jobs:
  performance:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable

      - name: Download baseline metrics
        run: |
          # Download from previous successful build
          curl -o baseline.json \
            https://...baseline-metrics-url.../large_scale_terrain_worldtest.json

      - name: Run worldtests and generate metrics
        run: |
          cargo test --package mdminecraft-world --test '*worldtest' -- --nocapture

      - name: Check for regressions
        run: |
          cargo run --package mdminecraft-cli --bin metrics-diff -- \
            baseline.json \
            target/metrics/large_scale_terrain_worldtest.json \
            --format json > diff-report.json

      - name: Upload diff report
        uses: actions/upload-artifact@v3
        with:
          name: performance-diff
          path: diff-report.json

      - name: Comment on PR (if regressions found)
        if: failure()
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const report = JSON.parse(fs.readFileSync('diff-report.json'));
            // Post comment with regression details
```

### GitLab CI Example

```yaml
performance_check:
  stage: test
  script:
    - cargo test --package mdminecraft-world --test '*worldtest' -- --nocapture
    - cargo run --package mdminecraft-cli --bin metrics-diff --
        baseline.json
        target/metrics/large_scale_terrain_worldtest.json
  artifacts:
    paths:
      - target/metrics/*.json
    reports:
      junit: test-results.xml
  allow_failure:
    exit_codes: 1  # Allow warnings
```

---

## Workflow Examples

### Local Development

**1. Establish baseline:**
```bash
# Run tests to generate metrics
cargo test --package mdminecraft-world --test large_scale_terrain_worldtest -- --nocapture

# Save as baseline
cp target/metrics/large_scale_terrain_worldtest.json baseline.json
```

**2. Make changes to code**

**3. Check for regressions:**
```bash
# Run tests again
cargo test --package mdminecraft-world --test large_scale_terrain_worldtest -- --nocapture

# Compare
metrics-diff baseline.json target/metrics/large_scale_terrain_worldtest.json
```

### Pre-Release Validation

```bash
#!/bin/bash
# pre-release-check.sh

echo "Running pre-release performance validation..."

# Download production baseline
curl -o baseline.json https://releases.example.com/baselines/v0.1.0.json

# Run all worldtests
cargo test --package mdminecraft-world --test '*worldtest' -- --nocapture

# Check each worldtest
for metrics_file in target/metrics/*.json; do
    echo "Checking $(basename $metrics_file)..."
    metrics-diff baseline.json "$metrics_file" || exit 1
done

echo "✅ All performance checks passed"
```

### Continuous Monitoring

```bash
#!/bin/bash
# Store historical metrics

DATE=$(date +%Y%m%d)
COMMIT=$(git rev-parse --short HEAD)

# Run tests
cargo test --package mdminecraft-world --test '*worldtest' -- --nocapture

# Archive metrics
mkdir -p metrics-history
cp target/metrics/*.json "metrics-history/${DATE}-${COMMIT}.json"

# Compare with previous day
if [ -f "metrics-history/latest.json" ]; then
    metrics-diff metrics-history/latest.json target/metrics/*.json
fi

# Update latest
cp target/metrics/*.json metrics-history/latest.json
```

---

## Troubleshooting

### Error: "Failed to read file"

**Cause:** File path is incorrect or file doesn't exist

**Solution:**
```bash
# Check file exists
ls -l baseline.json current.json

# Use absolute paths if needed
metrics-diff /full/path/to/baseline.json /full/path/to/current.json
```

### Error: "Failed to parse JSON"

**Cause:** Metrics file is corrupted or invalid JSON

**Solution:**
```bash
# Validate JSON
jq . baseline.json

# Check metrics file was generated correctly
cargo test --package mdminecraft-world --test determinism_worldtest -- --nocapture
```

### Unexpected Thresholds

**Cause:** Thresholds specified as percentages instead of decimals

**Solution:**
```bash
# WRONG: --threshold-warning 5 (means 500%)
# RIGHT: --threshold-warning 0.05 (means 5%)

metrics-diff baseline.json current.json --threshold-warning 0.05
```

### No Metrics to Compare

**Cause:** Baseline and current tests don't have overlapping metrics

**Solution:**
- Ensure both metrics files are from the same worldtest
- Check that both files have the same metric categories
- Run the same test for both baseline and current

---

## Best Practices

### Threshold Selection

**Conservative (for critical paths):**
- Warning: 3% (`--threshold-warning 0.03`)
- Failure: 5% (`--threshold-failure 0.05`)

**Standard (recommended):**
- Warning: 5% (`--threshold-warning 0.05`)
- Failure: 10% (`--threshold-failure 0.10`)

**Relaxed (for noisy tests):**
- Warning: 10% (`--threshold-warning 0.10`)
- Failure: 15% (`--threshold-failure 0.15`)

### Baseline Management

1. **Update baselines when improving performance**
   - Don't keep old baselines after optimizations
   - Document baseline updates in release notes

2. **Store baselines with version tags**
   - `baselines/v0.1.0.json`
   - `baselines/v0.2.0.json`

3. **Validate baselines are from clean builds**
   - Always use release builds for baselines
   - Ensure consistent hardware

### CI Integration Tips

1. **Store baselines as artifacts**
   - Upload baseline metrics from main branch
   - Download for PR comparisons

2. **Comment on PRs with results**
   - Show regression details in PR comments
   - Include links to full reports

3. **Allow warnings but fail on regressions**
   - Exit code 1 (warnings) = pass with note
   - Exit code 2 (failures) = block merge

4. **Track metrics over time**
   - Store historical metrics
   - Generate trend graphs
   - Alert on gradual degradation

---

## Advanced Usage

### Scripted Comparison of Multiple Tests

```bash
#!/bin/bash
TESTS="large_scale_terrain persistence_roundtrip mob_lifecycle determinism"

for test in $TESTS; do
    echo "Checking ${test}_worldtest..."
    metrics-diff \
        "baselines/${test}.json" \
        "target/metrics/${test}_worldtest.json" \
        --format json > "reports/${test}_diff.json"
done
```

### Generate HTML Report

```bash
#!/bin/bash
# Requires jq and basic HTML templating

metrics-diff baseline.json current.json --format json | \
  jq -r '"<tr><td>\(.name)</td><td>\(.baseline)</td><td>\(.current)</td><td>\(.change_percent)%</td></tr>"' | \
  cat <(echo "<table><tr><th>Metric</th><th>Baseline</th><th>Current</th><th>Change</th></tr>") - <(echo "</table>") > report.html
```

### Integration with Performance Dashboard

```bash
#!/bin/bash
# POST metrics diff to dashboard API

DIFF=$(metrics-diff baseline.json current.json --format json)

curl -X POST https://dashboard.example.com/api/metrics \
  -H "Content-Type: application/json" \
  -d "$DIFF"
```

---

## See Also

- [Performance Baselines](2025.11.15%20-%20BAS%20-%20Performance%20Baselines.md) - Performance targets and thresholds
- [Worldtest Usage Guide](2025.11.15%20-%20DOC%20-%20Worldtest%20Usage%20Guide.md) - How to run worldtests
- [Metrics Schema](../crates/testkit/src/metrics.rs) - Metrics data structure

---

**Document Version:** 1.0
**Last Updated:** 2025-11-15
**Tool Version:** 0.1.0
